{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.metrics import regression_coverage_score, regression_mean_width_score\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.subsample import Subsample\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/data.csv'\n",
    "data = pd.read_csv(path)\n",
    "normalized_pctg_change = data['normalized_percent_change'] # Save variable fot later use in model\n",
    "prediction = data['prediction'] \n",
    "data.drop(columns=['normalized_percent_change'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_df = data.copy()\n",
    "# One-hot encoding\n",
    "demographic_vars = ['gender_source_value', 'race_source_value', 'ethnicity_source_value']\n",
    "xgboost_df = pd.get_dummies(xgboost_df, columns=demographic_vars)\n",
    "# Scaling: Apparently there's no difference if a use a StandardScaler vs MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numeric_vars = ['mean_led_per_visit', 'age', 'length_of_stay', 'days_since_last_visit', 'days_to_diagnosis']\n",
    "for i in range(len(numeric_vars)):\n",
    "    xgboost_df[numeric_vars[i]] = scaler.fit_transform(xgboost_df[[numeric_vars[i]]])\n",
    "\n",
    "# Reordering the columns so that the target variable is the last one\n",
    "prediction_to_last = xgboost_df.pop('prediction')\n",
    "xgboost_df['prediction'] = prediction_to_last\n",
    "\n",
    "# Defining the features and target variable\n",
    "X = xgboost_df.iloc[:, :-1] # Shape is rows x features (38)\n",
    "y = xgboost_df.iloc[:, -1]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# I don't need to use cupy or cudf to send the data to the GPU because I'm using DMatrix\n",
    "random_state = 21\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "d_train = xgb.DMatrix(X_train, y_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, y_test, label=y_test)\n",
    "\n",
    "# Parameter previously calculated by doing hyperparameter tuning\n",
    "# Note: Accuracy drops with L2 regularization and higher depth. A smaller L1 regularization bumps the model\n",
    "# Note: When I avoid dropping nas in mean_led_per_patient accuracy bumps to 91%\n",
    "# Note: -> Remove the least importanf features in the model and run until you find the appropiate number of features\n",
    "# Model seems to perform better when I use train instead of XGBClassifier\n",
    "best_params = {\n",
    "    # Classification\n",
    "    \"eval_metric\": \"auc\", # Area under the curve\n",
    "    # \"eval_metric\": \"mlogloss\" For multiclass\n",
    "    \"objective\": \"binary:logistic\", # Logistic regression for binary classification, output probability\n",
    "    # \"objective\": \"multi:softprob\", # Multi-class classification\n",
    "    # 'num_class': 3, # Number of classes (required when doing multi-class classification)\n",
    "    # Regression:\n",
    "    # \"eval_metric\": 'rmse',\n",
    "    # 'objective': 'reg:squarederror',\n",
    "    'sampling_method': 'gradient_based', # The selection probability for each training instance is proportional to the regularized absolute value of gradients \n",
    "    'alpha': 0.1, # L1 regularization\n",
    "    'lambda': 1, # L2 regularization\n",
    "    'learning_rate': 0.1, \n",
    "    'max_depth': 7,\n",
    "    'num_boost_round': 700, \n",
    "    'tree_method': 'hist', \n",
    "    'device': \"cuda\",\n",
    "}\n",
    "num_boost_round = best_params['num_boost_round']\n",
    "# Prior AUC: 0.86066216116602046\n",
    "# Best new = 0.92090948515188154 difference is nwq \n",
    "regression_params_short = {'alpha': 0.1, 'lambda': 1, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 700, \"eval_metric\": 'rmse', 'objective': 'reg:squarederror', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\"} # Updated 28/08/2024. Range (0,1)\n",
    "regression_params_long = {'alpha': 1, 'lambda': 0.1, 'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 700, \"eval_metric\": 'rmse', 'objective': 'reg:squarederror', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\"} # Updated 01/09/2024. Range (-1,1) Decreases coverage in Jackknife+ by 0.02\n",
    "binary_params = {'alpha': 0, 'lambda': 0.1, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 800, \"eval_metric\": 'auc', 'objective': 'binary:logistic', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\",} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_five_equal_parts(X, y_binary, y_continuous, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into 5 equal parts, maintaining alignment between binary and continuous outcomes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features DataFrame\n",
    "    y_binary : binary outcome (change/no change in LEDD)\n",
    "    y_continuous : continuous outcome (normalized percentage change)\n",
    "    random_state : for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Training, calibration, and test sets for both binary and continuous outcomes\n",
    "    \"\"\"\n",
    "    # First split: 80% for train+calibration, 20% for test\n",
    "    X_temp, X_test, y_binary_temp, y_binary_test, y_cont_temp, y_cont_test = train_test_split(\n",
    "        X, y_binary, y_continuous, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split the remaining 80% into 4 equal parts\n",
    "    X_train, X_temp2, y_binary_train, y_binary_temp2, y_cont_train, y_cont_temp2 = train_test_split(\n",
    "        X_temp, y_binary_temp, y_cont_temp, test_size=0.75, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split the 60% into 3 equal calibration sets\n",
    "    X_calib1, X_temp3, y_binary_calib1, y_binary_temp3, y_cont_calib1, y_cont_temp3 = train_test_split(\n",
    "        X_temp2, y_binary_temp2, y_cont_temp2, test_size=0.666, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_calib2, X_calib3, y_binary_calib2, y_binary_calib3, y_cont_calib2, y_cont_calib3 = train_test_split(\n",
    "        X_temp3, y_binary_temp3, y_cont_temp3, test_size=0.5, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "        y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "        y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_zero_inflated(X, y_binary, y_continuous, alpha_tilda=0.9):\n",
    "    # Split data into five parts\n",
    "    (X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "     y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "     y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test) = split_into_five_equal_parts(\n",
    "        X, y_binary, y_continuous\n",
    "    )\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    d_train = xgb.DMatrix(X_train, label=y_binary_train)\n",
    "    d_calib1 = xgb.DMatrix(X_calib1, label=y_binary_calib1)\n",
    "    d_calib2 = xgb.DMatrix(X_calib2, label=y_binary_calib2)\n",
    "    d_calib3 = xgb.DMatrix(X_calib3, label=y_binary_calib3)\n",
    "    d_test = xgb.DMatrix(X_test, label=y_binary_test)\n",
    "    \n",
    "    # Train classification model on binary outcome\n",
    "    classifier = xgb.train(\n",
    "        params=binary_params,\n",
    "        dtrain=d_train,\n",
    "        num_boost_round=binary_params['n_estimators'],\n",
    "        evals=[(d_train, 'train')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # Train regression model on non-zero continuous outcomes\n",
    "    mask_nonzero = y_binary_train == 1\n",
    "    d_train_reg = xgb.DMatrix(X_train[mask_nonzero], label=y_cont_train[mask_nonzero])\n",
    "    regressor = xgb.train(\n",
    "        params=regression_params_short,\n",
    "        dtrain=d_train_reg,\n",
    "        num_boost_round=regression_params_short['n_estimators'],\n",
    "        evals=[(d_train_reg, 'train')],\n",
    "        early_stopping_rounds=10,\n",
    "        verbose_eval=False\n",
    "    )\n",
    "    \n",
    "    # First calibration set: Determine alpha_r\n",
    "    probs_calib1 = classifier.predict(d_calib1)\n",
    "    r_values = np.arange(0.1, 0.95, 0.05)\n",
    "    all_interval_lengths = []\n",
    "    \n",
    "    for r in r_values:\n",
    "        alpha_r = np.quantile(probs_calib1, r)\n",
    "        \n",
    "        # Second calibration set: Calculate beta_hat\n",
    "        probs_calib2 = classifier.predict(d_calib2)\n",
    "        pred_zeros = probs_calib2 <= alpha_r\n",
    "        beta_hat = np.mean(y_binary_calib2[pred_zeros] == 0)\n",
    "        \n",
    "        # Calculate final quantile\n",
    "        final_quantile = (alpha_tilda - beta_hat * r) / (1 - r)\n",
    "        if final_quantile < 0 or final_quantile > 1:\n",
    "            continue\n",
    "            \n",
    "        # Third calibration set: Calculate interval width\n",
    "        probs_calib3 = classifier.predict(d_calib3)\n",
    "        nonzero_mask = probs_calib3 > alpha_r\n",
    "        if not any(nonzero_mask):\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        d_calib3_reg = xgb.DMatrix(X_calib3[nonzero_mask])\n",
    "        y_pred_nonzero = regressor.predict(d_calib3_reg)\n",
    "        residuals = np.abs(y_cont_calib3[nonzero_mask] - y_pred_nonzero)\n",
    "        interval_width = np.quantile(residuals, final_quantile)\n",
    "        all_interval_lengths.append((r, interval_width, alpha_r))\n",
    "    \n",
    "    # Choose best r value and corresponding width\n",
    "    best_r, best_width, best_alpha_r = min(all_interval_lengths, key=lambda x: x[1])\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    test_probs = classifier.predict(d_test)\n",
    "    d_test_reg = xgb.DMatrix(X_test)\n",
    "    test_pred = regressor.predict(d_test_reg)\n",
    "    \n",
    "    # Create prediction intervals\n",
    "    lower_bound = np.zeros_like(test_pred)\n",
    "    upper_bound = np.zeros_like(test_pred)\n",
    "    nonzero_mask = test_probs > best_alpha_r\n",
    "    lower_bound[nonzero_mask] = test_pred[nonzero_mask] - best_width\n",
    "    upper_bound[nonzero_mask] = test_pred[nonzero_mask] + best_width\n",
    "    \n",
    "    return lower_bound, upper_bound, y_cont_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-auc:0.86294\n",
      "[1]\ttest-auc:0.86382\n",
      "[2]\ttest-auc:0.87933\n",
      "[3]\ttest-auc:0.88895\n",
      "[4]\ttest-auc:0.89025\n",
      "[5]\ttest-auc:0.89470\n",
      "[6]\ttest-auc:0.89596\n",
      "[7]\ttest-auc:0.89800\n",
      "[8]\ttest-auc:0.89995\n",
      "[9]\ttest-auc:0.90051\n",
      "[10]\ttest-auc:0.90198\n",
      "[11]\ttest-auc:0.90364\n",
      "[12]\ttest-auc:0.90498\n",
      "[13]\ttest-auc:0.90394\n",
      "[14]\ttest-auc:0.90669\n",
      "[15]\ttest-auc:0.90791\n",
      "[16]\ttest-auc:0.91025\n",
      "[17]\ttest-auc:0.91106\n",
      "[18]\ttest-auc:0.91245\n",
      "[19]\ttest-auc:0.91457\n",
      "[20]\ttest-auc:0.91526\n",
      "[21]\ttest-auc:0.91669\n",
      "[22]\ttest-auc:0.91825\n",
      "[23]\ttest-auc:0.91827\n",
      "[24]\ttest-auc:0.91895\n",
      "[25]\ttest-auc:0.91995\n",
      "[26]\ttest-auc:0.92116\n",
      "[27]\ttest-auc:0.92150\n",
      "[28]\ttest-auc:0.92219\n",
      "[29]\ttest-auc:0.92224\n",
      "[30]\ttest-auc:0.92256\n",
      "[31]\ttest-auc:0.92388\n",
      "[32]\ttest-auc:0.92452\n",
      "[33]\ttest-auc:0.92480\n",
      "[34]\ttest-auc:0.92626\n",
      "[35]\ttest-auc:0.92725\n",
      "[36]\ttest-auc:0.92796\n",
      "[37]\ttest-auc:0.92874\n",
      "[38]\ttest-auc:0.92923\n",
      "[39]\ttest-auc:0.92959\n",
      "[40]\ttest-auc:0.92964\n",
      "[41]\ttest-auc:0.92987\n",
      "[42]\ttest-auc:0.93013\n",
      "[43]\ttest-auc:0.93105\n",
      "[44]\ttest-auc:0.93176\n",
      "[45]\ttest-auc:0.93203\n",
      "[46]\ttest-auc:0.93258\n",
      "[47]\ttest-auc:0.93262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:50:12] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"num_boost_round\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\ttest-auc:0.93430\n",
      "[49]\ttest-auc:0.93468\n",
      "[50]\ttest-auc:0.93605\n",
      "[51]\ttest-auc:0.93615\n",
      "[52]\ttest-auc:0.93664\n",
      "[53]\ttest-auc:0.93707\n",
      "[54]\ttest-auc:0.93782\n",
      "[55]\ttest-auc:0.93816\n",
      "[56]\ttest-auc:0.93814\n",
      "[57]\ttest-auc:0.93839\n",
      "[58]\ttest-auc:0.93877\n",
      "[59]\ttest-auc:0.93946\n",
      "[60]\ttest-auc:0.93984\n",
      "[61]\ttest-auc:0.94022\n",
      "[62]\ttest-auc:0.94035\n",
      "[63]\ttest-auc:0.94063\n",
      "[64]\ttest-auc:0.94162\n",
      "[65]\ttest-auc:0.94183\n",
      "[66]\ttest-auc:0.94225\n",
      "[67]\ttest-auc:0.94252\n",
      "[68]\ttest-auc:0.94300\n",
      "[69]\ttest-auc:0.94326\n",
      "[70]\ttest-auc:0.94375\n",
      "[71]\ttest-auc:0.94465\n",
      "[72]\ttest-auc:0.94495\n",
      "[73]\ttest-auc:0.94504\n",
      "[74]\ttest-auc:0.94561\n",
      "[75]\ttest-auc:0.94609\n",
      "[76]\ttest-auc:0.94658\n",
      "[77]\ttest-auc:0.94715\n",
      "[78]\ttest-auc:0.94728\n",
      "[79]\ttest-auc:0.94766\n",
      "[80]\ttest-auc:0.94787\n",
      "[81]\ttest-auc:0.94791\n",
      "[82]\ttest-auc:0.94912\n",
      "[83]\ttest-auc:0.94931\n",
      "[84]\ttest-auc:0.94967\n",
      "[85]\ttest-auc:0.94969\n",
      "[86]\ttest-auc:0.94961\n",
      "[87]\ttest-auc:0.94970\n",
      "[88]\ttest-auc:0.94978\n",
      "[89]\ttest-auc:0.94994\n",
      "[90]\ttest-auc:0.95027\n",
      "[91]\ttest-auc:0.95048\n",
      "[92]\ttest-auc:0.95081\n",
      "[93]\ttest-auc:0.95106\n",
      "[94]\ttest-auc:0.95099\n",
      "[95]\ttest-auc:0.95132\n",
      "[96]\ttest-auc:0.95142\n",
      "[97]\ttest-auc:0.95171\n",
      "[98]\ttest-auc:0.95183\n",
      "[99]\ttest-auc:0.95161\n",
      "[100]\ttest-auc:0.95154\n",
      "[101]\ttest-auc:0.95172\n",
      "[102]\ttest-auc:0.95188\n",
      "[103]\ttest-auc:0.95239\n",
      "[104]\ttest-auc:0.95315\n",
      "[105]\ttest-auc:0.95327\n",
      "[106]\ttest-auc:0.95328\n",
      "[107]\ttest-auc:0.95354\n",
      "[108]\ttest-auc:0.95391\n",
      "[109]\ttest-auc:0.95391\n",
      "[110]\ttest-auc:0.95394\n",
      "[111]\ttest-auc:0.95393\n",
      "[112]\ttest-auc:0.95396\n",
      "[113]\ttest-auc:0.95409\n",
      "[114]\ttest-auc:0.95424\n",
      "[115]\ttest-auc:0.95417\n",
      "[116]\ttest-auc:0.95437\n",
      "[117]\ttest-auc:0.95438\n",
      "[118]\ttest-auc:0.95460\n",
      "[119]\ttest-auc:0.95458\n",
      "[120]\ttest-auc:0.95477\n",
      "[121]\ttest-auc:0.95486\n",
      "[122]\ttest-auc:0.95498\n",
      "[123]\ttest-auc:0.95550\n",
      "[124]\ttest-auc:0.95552\n",
      "[125]\ttest-auc:0.95565\n",
      "[126]\ttest-auc:0.95626\n",
      "[127]\ttest-auc:0.95634\n",
      "[128]\ttest-auc:0.95628\n",
      "[129]\ttest-auc:0.95653\n",
      "[130]\ttest-auc:0.95661\n",
      "[131]\ttest-auc:0.95719\n",
      "[132]\ttest-auc:0.95706\n",
      "[133]\ttest-auc:0.95714\n",
      "[134]\ttest-auc:0.95727\n",
      "[135]\ttest-auc:0.95741\n",
      "[136]\ttest-auc:0.95757\n",
      "[137]\ttest-auc:0.95758\n",
      "[138]\ttest-auc:0.95767\n",
      "[139]\ttest-auc:0.95795\n",
      "[140]\ttest-auc:0.95810\n",
      "[141]\ttest-auc:0.95839\n",
      "[142]\ttest-auc:0.95850\n",
      "[143]\ttest-auc:0.95850\n",
      "[144]\ttest-auc:0.95864\n",
      "[145]\ttest-auc:0.95867\n",
      "[146]\ttest-auc:0.95865\n",
      "[147]\ttest-auc:0.95899\n",
      "[148]\ttest-auc:0.95903\n",
      "[149]\ttest-auc:0.95957\n",
      "[150]\ttest-auc:0.95964\n",
      "[151]\ttest-auc:0.95976\n",
      "[152]\ttest-auc:0.95993\n",
      "[153]\ttest-auc:0.95997\n",
      "[154]\ttest-auc:0.95979\n",
      "[155]\ttest-auc:0.95995\n",
      "[156]\ttest-auc:0.96037\n",
      "[157]\ttest-auc:0.96043\n",
      "[158]\ttest-auc:0.96061\n",
      "[159]\ttest-auc:0.96064\n",
      "[160]\ttest-auc:0.96087\n",
      "[161]\ttest-auc:0.96103\n",
      "[162]\ttest-auc:0.96098\n",
      "[163]\ttest-auc:0.96113\n",
      "[164]\ttest-auc:0.96107\n",
      "[165]\ttest-auc:0.96119\n",
      "[166]\ttest-auc:0.96131\n",
      "[167]\ttest-auc:0.96148\n",
      "[168]\ttest-auc:0.96182\n",
      "[169]\ttest-auc:0.96189\n",
      "[170]\ttest-auc:0.96181\n",
      "[171]\ttest-auc:0.96181\n",
      "[172]\ttest-auc:0.96178\n",
      "[173]\ttest-auc:0.96176\n",
      "[174]\ttest-auc:0.96194\n",
      "[175]\ttest-auc:0.96212\n",
      "[176]\ttest-auc:0.96213\n",
      "[177]\ttest-auc:0.96213\n",
      "[178]\ttest-auc:0.96224\n",
      "[179]\ttest-auc:0.96234\n",
      "[180]\ttest-auc:0.96254\n",
      "[181]\ttest-auc:0.96254\n",
      "[182]\ttest-auc:0.96255\n",
      "[183]\ttest-auc:0.96242\n",
      "[184]\ttest-auc:0.96256\n",
      "[185]\ttest-auc:0.96262\n",
      "[186]\ttest-auc:0.96259\n",
      "[187]\ttest-auc:0.96264\n",
      "[188]\ttest-auc:0.96268\n",
      "[189]\ttest-auc:0.96275\n",
      "[190]\ttest-auc:0.96265\n",
      "[191]\ttest-auc:0.96269\n",
      "[192]\ttest-auc:0.96280\n",
      "[193]\ttest-auc:0.96277\n",
      "[194]\ttest-auc:0.96284\n",
      "[195]\ttest-auc:0.96293\n",
      "[196]\ttest-auc:0.96293\n",
      "[197]\ttest-auc:0.96309\n",
      "[198]\ttest-auc:0.96315\n",
      "[199]\ttest-auc:0.96309\n",
      "[200]\ttest-auc:0.96313\n",
      "[201]\ttest-auc:0.96322\n",
      "[202]\ttest-auc:0.96323\n",
      "[203]\ttest-auc:0.96318\n",
      "[204]\ttest-auc:0.96314\n",
      "[205]\ttest-auc:0.96328\n",
      "[206]\ttest-auc:0.96326\n",
      "[207]\ttest-auc:0.96321\n",
      "[208]\ttest-auc:0.96320\n",
      "[209]\ttest-auc:0.96328\n",
      "[210]\ttest-auc:0.96330\n",
      "[211]\ttest-auc:0.96341\n",
      "[212]\ttest-auc:0.96348\n",
      "[213]\ttest-auc:0.96351\n",
      "[214]\ttest-auc:0.96350\n",
      "[215]\ttest-auc:0.96349\n",
      "[216]\ttest-auc:0.96356\n",
      "[217]\ttest-auc:0.96360\n",
      "[218]\ttest-auc:0.96361\n",
      "[219]\ttest-auc:0.96361\n",
      "[220]\ttest-auc:0.96358\n",
      "[221]\ttest-auc:0.96367\n",
      "[222]\ttest-auc:0.96387\n",
      "[223]\ttest-auc:0.96392\n",
      "[224]\ttest-auc:0.96387\n",
      "[225]\ttest-auc:0.96398\n",
      "[226]\ttest-auc:0.96391\n",
      "[227]\ttest-auc:0.96390\n",
      "[228]\ttest-auc:0.96397\n",
      "[229]\ttest-auc:0.96402\n",
      "[230]\ttest-auc:0.96404\n",
      "[231]\ttest-auc:0.96414\n",
      "[232]\ttest-auc:0.96426\n",
      "[233]\ttest-auc:0.96416\n",
      "[234]\ttest-auc:0.96409\n",
      "[235]\ttest-auc:0.96398\n",
      "[236]\ttest-auc:0.96396\n",
      "[237]\ttest-auc:0.96391\n",
      "[238]\ttest-auc:0.96410\n",
      "[239]\ttest-auc:0.96418\n",
      "[240]\ttest-auc:0.96422\n",
      "[241]\ttest-auc:0.96421\n",
      "[242]\ttest-auc:0.96421\n"
     ]
    }
   ],
   "source": [
    "d_all = xgb.DMatrix(X)\n",
    "model = xgb.train(best_params, d_train, num_boost_round=num_boost_round, evals=((d_test, \"test\"),),verbose_eval=True, early_stopping_rounds=10)\n",
    "y_pred_proba_all = model.predict(d_all, iteration_range=(0, model.best_iteration + 1))\n",
    "y_pred_all = (y_pred_proba_all > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:50:13] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:50:14] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.841\n",
      "Average interval width: 0.156\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound, y_test = conformal_prediction_zero_inflated(\n",
    "    X=X,\n",
    "    y_binary=y_pred_all,\n",
    "    y_continuous=normalized_pctg_change,\n",
    "    alpha_tilda=0.9  # Target coverage (90%)\n",
    ")\n",
    "\n",
    "# Calculate and print coverage\n",
    "coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
    "interval_width = np.mean(upper_bound - lower_bound)\n",
    "print(f\"Coverage: {coverage:.3f}\")\n",
    "print(f\"Average interval width: {interval_width:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
