{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.metrics import regression_coverage_score, regression_mean_width_score\n",
    "from mapie.regression import MapieRegressor\n",
    "from mapie.subsample import Subsample\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/data.csv'\n",
    "data = pd.read_csv(path)\n",
    "normalized_pctg_change = data['normalized_percent_change'] # Save variable fot later use in model\n",
    "prediction = data['prediction'] \n",
    "data.drop(columns=['normalized_percent_change'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-auc:0.86294\n",
      "[1]\ttest-auc:0.86382\n",
      "[2]\ttest-auc:0.87933\n",
      "[3]\ttest-auc:0.88895\n",
      "[4]\ttest-auc:0.89025\n",
      "[5]\ttest-auc:0.89470\n",
      "[6]\ttest-auc:0.89596\n",
      "[7]\ttest-auc:0.89800\n",
      "[8]\ttest-auc:0.89995\n",
      "[9]\ttest-auc:0.90051\n",
      "[10]\ttest-auc:0.90198\n",
      "[11]\ttest-auc:0.90364\n",
      "[12]\ttest-auc:0.90498\n",
      "[13]\ttest-auc:0.90394\n",
      "[14]\ttest-auc:0.90669\n",
      "[15]\ttest-auc:0.90791\n",
      "[16]\ttest-auc:0.91025\n",
      "[17]\ttest-auc:0.91106\n",
      "[18]\ttest-auc:0.91245\n",
      "[19]\ttest-auc:0.91457\n",
      "[20]\ttest-auc:0.91526\n",
      "[21]\ttest-auc:0.91669\n",
      "[22]\ttest-auc:0.91825\n",
      "[23]\ttest-auc:0.91827\n",
      "[24]\ttest-auc:0.91895\n",
      "[25]\ttest-auc:0.91995\n",
      "[26]\ttest-auc:0.92116\n",
      "[27]\ttest-auc:0.92150\n",
      "[28]\ttest-auc:0.92219\n",
      "[29]\ttest-auc:0.92224\n",
      "[30]\ttest-auc:0.92256\n",
      "[31]\ttest-auc:0.92388\n",
      "[32]\ttest-auc:0.92452\n",
      "[33]\ttest-auc:0.92480\n",
      "[34]\ttest-auc:0.92626\n",
      "[35]\ttest-auc:0.92725\n",
      "[36]\ttest-auc:0.92796\n",
      "[37]\ttest-auc:0.92874\n",
      "[38]\ttest-auc:0.92923\n",
      "[39]\ttest-auc:0.92959\n",
      "[40]\ttest-auc:0.92964\n",
      "[41]\ttest-auc:0.92987\n",
      "[42]\ttest-auc:0.93013\n",
      "[43]\ttest-auc:0.93105\n",
      "[44]\ttest-auc:0.93176\n",
      "[45]\ttest-auc:0.93203\n",
      "[46]\ttest-auc:0.93258\n",
      "[47]\ttest-auc:0.93262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [21:56:08] WARNING: /workspace/src/learner.cc:742: \n",
      "Parameters: { \"num_boost_round\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[48]\ttest-auc:0.93430\n",
      "[49]\ttest-auc:0.93468\n",
      "[50]\ttest-auc:0.93605\n",
      "[51]\ttest-auc:0.93615\n",
      "[52]\ttest-auc:0.93664\n",
      "[53]\ttest-auc:0.93707\n",
      "[54]\ttest-auc:0.93782\n",
      "[55]\ttest-auc:0.93816\n",
      "[56]\ttest-auc:0.93814\n",
      "[57]\ttest-auc:0.93839\n",
      "[58]\ttest-auc:0.93877\n",
      "[59]\ttest-auc:0.93946\n",
      "[60]\ttest-auc:0.93984\n",
      "[61]\ttest-auc:0.94022\n",
      "[62]\ttest-auc:0.94035\n",
      "[63]\ttest-auc:0.94063\n",
      "[64]\ttest-auc:0.94162\n",
      "[65]\ttest-auc:0.94183\n",
      "[66]\ttest-auc:0.94225\n",
      "[67]\ttest-auc:0.94252\n",
      "[68]\ttest-auc:0.94300\n",
      "[69]\ttest-auc:0.94326\n",
      "[70]\ttest-auc:0.94375\n",
      "[71]\ttest-auc:0.94465\n",
      "[72]\ttest-auc:0.94495\n",
      "[73]\ttest-auc:0.94504\n",
      "[74]\ttest-auc:0.94561\n",
      "[75]\ttest-auc:0.94609\n",
      "[76]\ttest-auc:0.94658\n",
      "[77]\ttest-auc:0.94715\n",
      "[78]\ttest-auc:0.94728\n",
      "[79]\ttest-auc:0.94766\n",
      "[80]\ttest-auc:0.94787\n",
      "[81]\ttest-auc:0.94791\n",
      "[82]\ttest-auc:0.94912\n",
      "[83]\ttest-auc:0.94931\n",
      "[84]\ttest-auc:0.94967\n",
      "[85]\ttest-auc:0.94969\n",
      "[86]\ttest-auc:0.94961\n",
      "[87]\ttest-auc:0.94970\n",
      "[88]\ttest-auc:0.94978\n",
      "[89]\ttest-auc:0.94994\n",
      "[90]\ttest-auc:0.95027\n",
      "[91]\ttest-auc:0.95048\n",
      "[92]\ttest-auc:0.95081\n",
      "[93]\ttest-auc:0.95106\n",
      "[94]\ttest-auc:0.95099\n",
      "[95]\ttest-auc:0.95132\n",
      "[96]\ttest-auc:0.95142\n",
      "[97]\ttest-auc:0.95171\n",
      "[98]\ttest-auc:0.95183\n",
      "[99]\ttest-auc:0.95161\n",
      "[100]\ttest-auc:0.95154\n",
      "[101]\ttest-auc:0.95172\n",
      "[102]\ttest-auc:0.95188\n",
      "[103]\ttest-auc:0.95239\n",
      "[104]\ttest-auc:0.95315\n",
      "[105]\ttest-auc:0.95327\n",
      "[106]\ttest-auc:0.95328\n",
      "[107]\ttest-auc:0.95354\n",
      "[108]\ttest-auc:0.95391\n",
      "[109]\ttest-auc:0.95391\n",
      "[110]\ttest-auc:0.95394\n",
      "[111]\ttest-auc:0.95393\n",
      "[112]\ttest-auc:0.95396\n",
      "[113]\ttest-auc:0.95409\n",
      "[114]\ttest-auc:0.95424\n",
      "[115]\ttest-auc:0.95417\n",
      "[116]\ttest-auc:0.95437\n",
      "[117]\ttest-auc:0.95438\n",
      "[118]\ttest-auc:0.95460\n",
      "[119]\ttest-auc:0.95458\n",
      "[120]\ttest-auc:0.95477\n",
      "[121]\ttest-auc:0.95486\n",
      "[122]\ttest-auc:0.95498\n",
      "[123]\ttest-auc:0.95550\n",
      "[124]\ttest-auc:0.95552\n",
      "[125]\ttest-auc:0.95565\n",
      "[126]\ttest-auc:0.95626\n",
      "[127]\ttest-auc:0.95634\n",
      "[128]\ttest-auc:0.95628\n",
      "[129]\ttest-auc:0.95653\n",
      "[130]\ttest-auc:0.95661\n",
      "[131]\ttest-auc:0.95719\n",
      "[132]\ttest-auc:0.95706\n",
      "[133]\ttest-auc:0.95714\n",
      "[134]\ttest-auc:0.95727\n",
      "[135]\ttest-auc:0.95741\n",
      "[136]\ttest-auc:0.95757\n",
      "[137]\ttest-auc:0.95758\n",
      "[138]\ttest-auc:0.95767\n",
      "[139]\ttest-auc:0.95795\n",
      "[140]\ttest-auc:0.95810\n",
      "[141]\ttest-auc:0.95839\n",
      "[142]\ttest-auc:0.95850\n",
      "[143]\ttest-auc:0.95850\n",
      "[144]\ttest-auc:0.95864\n",
      "[145]\ttest-auc:0.95867\n",
      "[146]\ttest-auc:0.95865\n",
      "[147]\ttest-auc:0.95899\n",
      "[148]\ttest-auc:0.95903\n",
      "[149]\ttest-auc:0.95957\n",
      "[150]\ttest-auc:0.95964\n",
      "[151]\ttest-auc:0.95976\n",
      "[152]\ttest-auc:0.95993\n",
      "[153]\ttest-auc:0.95997\n",
      "[154]\ttest-auc:0.95979\n",
      "[155]\ttest-auc:0.95995\n",
      "[156]\ttest-auc:0.96037\n",
      "[157]\ttest-auc:0.96043\n",
      "[158]\ttest-auc:0.96061\n",
      "[159]\ttest-auc:0.96064\n",
      "[160]\ttest-auc:0.96087\n",
      "[161]\ttest-auc:0.96103\n",
      "[162]\ttest-auc:0.96098\n",
      "[163]\ttest-auc:0.96113\n",
      "[164]\ttest-auc:0.96107\n",
      "[165]\ttest-auc:0.96119\n",
      "[166]\ttest-auc:0.96131\n",
      "[167]\ttest-auc:0.96148\n",
      "[168]\ttest-auc:0.96182\n",
      "[169]\ttest-auc:0.96189\n",
      "[170]\ttest-auc:0.96181\n",
      "[171]\ttest-auc:0.96181\n",
      "[172]\ttest-auc:0.96178\n",
      "[173]\ttest-auc:0.96176\n",
      "[174]\ttest-auc:0.96194\n",
      "[175]\ttest-auc:0.96212\n",
      "[176]\ttest-auc:0.96213\n",
      "[177]\ttest-auc:0.96213\n",
      "[178]\ttest-auc:0.96224\n",
      "[179]\ttest-auc:0.96234\n",
      "[180]\ttest-auc:0.96254\n",
      "[181]\ttest-auc:0.96254\n",
      "[182]\ttest-auc:0.96255\n",
      "[183]\ttest-auc:0.96242\n",
      "[184]\ttest-auc:0.96256\n",
      "[185]\ttest-auc:0.96262\n",
      "[186]\ttest-auc:0.96259\n",
      "[187]\ttest-auc:0.96264\n",
      "[188]\ttest-auc:0.96268\n",
      "[189]\ttest-auc:0.96275\n",
      "[190]\ttest-auc:0.96265\n",
      "[191]\ttest-auc:0.96269\n",
      "[192]\ttest-auc:0.96280\n",
      "[193]\ttest-auc:0.96277\n",
      "[194]\ttest-auc:0.96284\n",
      "[195]\ttest-auc:0.96293\n",
      "[196]\ttest-auc:0.96293\n",
      "[197]\ttest-auc:0.96309\n",
      "[198]\ttest-auc:0.96315\n",
      "[199]\ttest-auc:0.96309\n",
      "[200]\ttest-auc:0.96313\n",
      "[201]\ttest-auc:0.96322\n",
      "[202]\ttest-auc:0.96323\n",
      "[203]\ttest-auc:0.96318\n",
      "[204]\ttest-auc:0.96314\n",
      "[205]\ttest-auc:0.96328\n",
      "[206]\ttest-auc:0.96326\n",
      "[207]\ttest-auc:0.96321\n",
      "[208]\ttest-auc:0.96320\n",
      "[209]\ttest-auc:0.96328\n",
      "[210]\ttest-auc:0.96330\n",
      "[211]\ttest-auc:0.96341\n",
      "[212]\ttest-auc:0.96348\n",
      "[213]\ttest-auc:0.96351\n",
      "[214]\ttest-auc:0.96350\n",
      "[215]\ttest-auc:0.96349\n",
      "[216]\ttest-auc:0.96356\n",
      "[217]\ttest-auc:0.96360\n",
      "[218]\ttest-auc:0.96361\n",
      "[219]\ttest-auc:0.96361\n",
      "[220]\ttest-auc:0.96358\n",
      "[221]\ttest-auc:0.96367\n",
      "[222]\ttest-auc:0.96387\n",
      "[223]\ttest-auc:0.96392\n",
      "[224]\ttest-auc:0.96387\n",
      "[225]\ttest-auc:0.96398\n",
      "[226]\ttest-auc:0.96391\n",
      "[227]\ttest-auc:0.96390\n",
      "[228]\ttest-auc:0.96397\n",
      "[229]\ttest-auc:0.96402\n",
      "[230]\ttest-auc:0.96404\n",
      "[231]\ttest-auc:0.96414\n",
      "[232]\ttest-auc:0.96426\n",
      "[233]\ttest-auc:0.96416\n",
      "[234]\ttest-auc:0.96409\n",
      "[235]\ttest-auc:0.96398\n",
      "[236]\ttest-auc:0.96396\n",
      "[237]\ttest-auc:0.96391\n",
      "[238]\ttest-auc:0.96410\n",
      "[239]\ttest-auc:0.96418\n",
      "[240]\ttest-auc:0.96422\n",
      "[241]\ttest-auc:0.96421\n"
     ]
    }
   ],
   "source": [
    "xgboost_df = data.copy()\n",
    "# One-hot encoding\n",
    "demographic_vars = ['gender_source_value', 'race_source_value', 'ethnicity_source_value']\n",
    "xgboost_df = pd.get_dummies(xgboost_df, columns=demographic_vars)\n",
    "# Scaling: Apparently there's no difference if a use a StandardScaler vs MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "numeric_vars = ['mean_led_per_visit', 'age', 'length_of_stay', 'days_since_last_visit', 'days_to_diagnosis']\n",
    "for i in range(len(numeric_vars)):\n",
    "    xgboost_df[numeric_vars[i]] = scaler.fit_transform(xgboost_df[[numeric_vars[i]]])\n",
    "\n",
    "# Reordering the columns so that the target variable is the last one\n",
    "prediction_to_last = xgboost_df.pop('prediction')\n",
    "xgboost_df['prediction'] = prediction_to_last\n",
    "\n",
    "# Defining the features and target variable\n",
    "X = xgboost_df.iloc[:, :-1] # Shape is rows x features (38)\n",
    "y = xgboost_df.iloc[:, -1]\n",
    "\n",
    "# Split data into training and testing sets\n",
    "# I don't need to use cupy or cudf to send the data to the GPU because I'm using DMatrix\n",
    "random_state = 21\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "d_train = xgb.DMatrix(X_train, y_train, label=y_train)\n",
    "d_test = xgb.DMatrix(X_test, y_test, label=y_test)\n",
    "best_params = {\n",
    "    # Classification\n",
    "    \"eval_metric\": \"auc\", # Area under the curve\n",
    "    \"objective\": \"binary:logistic\", # Logistic regression for binary classification, output probability\n",
    "    'sampling_method': 'gradient_based', # The selection probability for each training instance is proportional to the regularized absolute value of gradients \n",
    "    'alpha': 0.1, # L1 regularization\n",
    "    'lambda': 1, # L2 regularization\n",
    "    'learning_rate': 0.1, \n",
    "    'max_depth': 7,\n",
    "    'num_boost_round': 700, \n",
    "    'tree_method': 'hist', \n",
    "    'device': \"cuda\",\n",
    "}\n",
    "num_boost_round = best_params['num_boost_round']\n",
    "# Prior AUC: 0.86066216116602046\n",
    "# Best new = 0.92090948515188154 difference is nwq \n",
    "regression_params_short = {'alpha': 0.1, 'lambda': 1, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 700, \"eval_metric\": 'rmse', 'objective': 'reg:squarederror', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\"} # Updated 28/08/2024. Range (0,1)\n",
    "regression_params_long = {'alpha': 1, 'lambda': 0.1, 'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 700, \"eval_metric\": 'rmse', 'objective': 'reg:squarederror', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\"} # Updated 01/09/2024. Range (-1,1) Decreases coverage in Jackknife+ by 0.02\n",
    "binary_params = {'alpha': 0, 'lambda': 0.1, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 800, \"eval_metric\": 'auc', 'objective': 'binary:logistic', 'sampling_method': 'gradient_based', 'tree_method': 'hist', 'device': \"cuda\",} \n",
    "\n",
    "d_all = xgb.DMatrix(X)\n",
    "model = xgb.train(best_params, d_train, num_boost_round=num_boost_round, evals=((d_test, \"test\"),),verbose_eval=True, early_stopping_rounds=10)\n",
    "y_pred_proba_all = model.predict(d_all, iteration_range=(0, model.best_iteration + 1))\n",
    "y_pred_all = (y_pred_proba_all > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_five_equal_parts(X, y_binary, y_continuous, random_state=42):\n",
    "    \"\"\"\n",
    "    Split data into 5 equal parts, maintaining alignment between binary and continuous outcomes\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : features DataFrame\n",
    "    y_binary : binary outcome (change/no change in LEDD)\n",
    "    y_continuous : continuous outcome (normalized percentage change)\n",
    "    random_state : for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Training, calibration, and test sets for both binary and continuous outcomes\n",
    "    \"\"\"\n",
    "    # First split: 80% for train+calibration, 20% for test\n",
    "    X_temp, X_test, y_binary_temp, y_binary_test, y_cont_temp, y_cont_test = train_test_split(\n",
    "        X, y_binary, y_continuous, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split the remaining 80% into 4 equal parts\n",
    "    X_train, X_temp2, y_binary_train, y_binary_temp2, y_cont_train, y_cont_temp2 = train_test_split(\n",
    "        X_temp, y_binary_temp, y_cont_temp, test_size=0.75, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Split the 60% into 3 equal calibration sets\n",
    "    X_calib1, X_temp3, y_binary_calib1, y_binary_temp3, y_cont_calib1, y_cont_temp3 = train_test_split(\n",
    "        X_temp2, y_binary_temp2, y_cont_temp2, test_size=0.666, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    X_calib2, X_calib3, y_binary_calib2, y_binary_calib3, y_cont_calib2, y_cont_calib3 = train_test_split(\n",
    "        X_temp3, y_binary_temp3, y_cont_temp3, test_size=0.5, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "        y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "        y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_zero_inflated(X, y_binary, y_continuous, alpha_tilda=0.9):\n",
    "    # Split data into five parts\n",
    "    (X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "     y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "     y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test) = split_into_five_equal_parts(\n",
    "        X, y_binary, y_continuous\n",
    "    )\n",
    "    \n",
    "    # Train classification model on binary outcome\n",
    "    classifier = xgb.XGBClassifier(**binary_params)\n",
    "    classifier.fit(X_train, y_binary_train)\n",
    "    \n",
    "    # Train regression model on non-zero continuous outcomes\n",
    "    mask_nonzero = y_binary_train == 1\n",
    "    regressor = xgb.XGBRegressor(**regression_params_short)\n",
    "    regressor.fit(X_train[mask_nonzero], y_cont_train[mask_nonzero])\n",
    "    \n",
    "    # First calibration set: Determine alpha_r\n",
    "    probs_calib1 = classifier.predict_proba(X_calib1)[:, 1]\n",
    "    r_values = np.arange(0.1, 0.95, 0.05)\n",
    "    all_interval_lengths = []\n",
    "    \n",
    "    for r in r_values:\n",
    "        alpha_r = np.quantile(probs_calib1, r)\n",
    "        \n",
    "        # Second calibration set: Calculate beta_hat\n",
    "        probs_calib2 = classifier.predict_proba(X_calib2)[:, 1]\n",
    "        pred_zeros = probs_calib2 <= alpha_r\n",
    "        beta_hat = np.mean(y_binary_calib2[pred_zeros] == 0)\n",
    "        \n",
    "        # Calculate final quantile\n",
    "        final_quantile = (alpha_tilda - beta_hat * r) / (1 - r)\n",
    "        if final_quantile < 0 or final_quantile > 1:\n",
    "            continue\n",
    "            \n",
    "        # Third calibration set: Calculate interval width\n",
    "        probs_calib3 = classifier.predict_proba(X_calib3)[:, 1]\n",
    "        nonzero_mask = probs_calib3 > alpha_r\n",
    "        if not any(nonzero_mask):\n",
    "            continue\n",
    "            \n",
    "        y_pred_nonzero = regressor.predict(X_calib3[nonzero_mask])\n",
    "        residuals = np.abs(y_cont_calib3[nonzero_mask] - y_pred_nonzero)\n",
    "        interval_width = np.quantile(residuals, final_quantile)\n",
    "        all_interval_lengths.append((r, interval_width, alpha_r))\n",
    "    \n",
    "    # Choose best r value and corresponding width\n",
    "    best_r, best_width, best_alpha_r = min(all_interval_lengths, key=lambda x: x[1])\n",
    "    \n",
    "    # Make predictions on test set\n",
    "    test_probs = classifier.predict_proba(X_test)[:, 1]\n",
    "    test_pred = regressor.predict(X_test)\n",
    "    \n",
    "    # Create prediction intervals\n",
    "    lower_bound = np.zeros_like(test_pred)\n",
    "    upper_bound = np.zeros_like(test_pred)\n",
    "    nonzero_mask = test_probs > best_alpha_r\n",
    "    \n",
    "    lower_bound[nonzero_mask] = test_pred[nonzero_mask] - best_width\n",
    "    upper_bound[nonzero_mask] = test_pred[nonzero_mask] + best_width\n",
    "    \n",
    "    return lower_bound, upper_bound, y_cont_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coverage: 0.845\n",
      "Average interval width: 0.139\n"
     ]
    }
   ],
   "source": [
    "lower_bound, upper_bound, y_test = conformal_prediction_zero_inflated(\n",
    "    X=X,\n",
    "    y_binary=y_pred_all,\n",
    "    y_continuous=normalized_pctg_change,\n",
    "    alpha_tilda=0.9  # Target coverage (90%)\n",
    ")\n",
    "\n",
    "# Calculate and print coverage\n",
    "coverage = np.mean((y_test >= lower_bound) & (y_test <= upper_bound))\n",
    "interval_width = np.mean(upper_bound - lower_bound)\n",
    "print(f\"Coverage: {coverage:.3f}\")\n",
    "print(f\"Average interval width: {interval_width:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPIE Coverage: 0.735\n",
      "MAPIE Average interval width: 0.003\n"
     ]
    }
   ],
   "source": [
    "def conformal_prediction_zero_inflated_mapie(X, y_binary, y_continuous, alpha_tilda=0.9):\n",
    "    \"\"\"\n",
    "    Two-stage conformal prediction using MAPIE for regression intervals.\n",
    "    First stage: XGBoost classifier to predict zero/non-zero\n",
    "    Second stage: MAPIE with XGBoost regressor for prediction intervals\n",
    "    \"\"\"\n",
    "    # Split data into five parts\n",
    "    (X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "     y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "     y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test) = split_into_five_equal_parts(\n",
    "        X, y_binary, y_continuous\n",
    "    )\n",
    "    \n",
    "    # First stage: Classification model\n",
    "    classifier = xgb.XGBClassifier(**binary_params)\n",
    "    classifier.fit(X_train, y_binary_train)\n",
    "    \n",
    "    # Get probabilities for calibration set 1\n",
    "    probs_calib1 = classifier.predict_proba(X_calib1)[:, 1]\n",
    "    \n",
    "    # Try different r values\n",
    "    r_values = np.arange(0.1, 0.95, 0.05)\n",
    "    all_interval_lengths = []\n",
    "    \n",
    "    # Initialize MAPIE with XGBoost regressor\n",
    "    regressor = xgb.XGBRegressor(**regression_params_short)\n",
    "    mapie = MapieRegressor(\n",
    "        estimator=regressor,\n",
    "        method=\"plus\",  # Jack-knife+ method\n",
    "        cv=\"prefit\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Fit regressor on non-zero training data\n",
    "    mask_nonzero = y_binary_train == 1\n",
    "    regressor.fit(X_train[mask_nonzero], y_cont_train[mask_nonzero])\n",
    "    \n",
    "    for r in r_values:\n",
    "        # First calibration: threshold for zero/non-zero\n",
    "        alpha_r = np.quantile(probs_calib1, r)\n",
    "        \n",
    "        # Second calibration: accuracy of zero predictions\n",
    "        probs_calib2 = classifier.predict_proba(X_calib2)[:, 1]\n",
    "        pred_zeros = probs_calib2 <= alpha_r\n",
    "        beta_hat = np.mean(y_binary_calib2[pred_zeros] == 0)\n",
    "        \n",
    "        # Calculate final quantile\n",
    "        final_quantile = (alpha_tilda - beta_hat * r) / (1 - r)\n",
    "        if final_quantile < 0 or final_quantile > 1:\n",
    "            continue\n",
    "            \n",
    "        # Third calibration: Use MAPIE for interval width\n",
    "        probs_calib3 = classifier.predict_proba(X_calib3)[:, 1]\n",
    "        nonzero_mask = probs_calib3 > alpha_r\n",
    "        if not any(nonzero_mask):\n",
    "            continue\n",
    "        \n",
    "        # Fit MAPIE on non-zero calibration data\n",
    "        X_calib3_nonzero = X_calib3[nonzero_mask]\n",
    "        y_calib3_nonzero = y_cont_calib3[nonzero_mask]\n",
    "        \n",
    "        # Get MAPIE prediction intervals\n",
    "        mapie.fit(X_calib3_nonzero, y_calib3_nonzero)\n",
    "        _, y_pis = mapie.predict(X_calib3_nonzero, alpha=final_quantile)\n",
    "        \n",
    "        # Calculate interval width as mean of MAPIE intervals\n",
    "        interval_width = np.mean(y_pis[:, 1, 0] - y_pis[:, 0, 0])\n",
    "        all_interval_lengths.append((r, interval_width, alpha_r))\n",
    "    \n",
    "    if not all_interval_lengths:\n",
    "        raise ValueError(\"No valid intervals found. Try adjusting r_values or alpha_tilda.\")\n",
    "    \n",
    "    # Choose best r value\n",
    "    best_r, best_width, best_alpha_r = min(all_interval_lengths, key=lambda x: x[1])\n",
    "    \n",
    "    # Final predictions on test set\n",
    "    test_probs = classifier.predict_proba(X_test)[:, 1]\n",
    "    nonzero_mask_test = test_probs > best_alpha_r\n",
    "    \n",
    "    # Initialize bounds\n",
    "    lower_bound = np.zeros(len(X_test))\n",
    "    upper_bound = np.zeros(len(X_test))\n",
    "    \n",
    "    if any(nonzero_mask_test):\n",
    "        # Get MAPIE predictions for non-zero cases\n",
    "        _, y_pis_test = mapie.predict(X_test[nonzero_mask_test], alpha=0.8)\n",
    "        lower_bound[nonzero_mask_test] = y_pis_test[:, 0, 0]\n",
    "        upper_bound[nonzero_mask_test] = y_pis_test[:, 1, 0]\n",
    "    \n",
    "    return lower_bound, upper_bound, y_cont_test\n",
    "\n",
    "# Usage\n",
    "lower_bound_mapie, upper_bound_mapie, y_test_mapie = conformal_prediction_zero_inflated_mapie(\n",
    "    X=X,\n",
    "    y_binary=y_pred_all,\n",
    "    y_continuous=normalized_pctg_change,\n",
    "    alpha_tilda=0.9\n",
    ")\n",
    "\n",
    "# Calculate coverage and interval width\n",
    "coverage_mapie = np.mean((y_test_mapie >= lower_bound_mapie) & (y_test_mapie <= upper_bound_mapie))\n",
    "interval_width_mapie = np.mean(upper_bound_mapie - lower_bound_mapie)\n",
    "print(f\"MAPIE Coverage: {coverage_mapie:.3f}\")\n",
    "print(f\"MAPIE Average interval width: {interval_width_mapie:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conformal_prediction_zero_inflated_mapie(X, y_binary, y_continuous, alpha_tilda=0.9):\n",
    "    \"\"\"\n",
    "    Two-stage conformal prediction using MAPIE for regression intervals.\n",
    "    \"\"\"\n",
    "    # Split data into five parts\n",
    "    (X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "     y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "     y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test) = split_into_five_equal_parts(\n",
    "        X, y_binary, y_continuous\n",
    "    )\n",
    "    \n",
    "    # First stage: Classification model\n",
    "    classifier = xgb.XGBClassifier(**binary_params)\n",
    "    classifier.fit(X_train, y_binary_train)\n",
    "    \n",
    "    # Get probabilities for calibration set 1\n",
    "    probs_calib1 = classifier.predict_proba(X_calib1)[:, 1]\n",
    "    \n",
    "    # Try different r values\n",
    "    r_values = np.arange(0.1, 0.95, 0.05)\n",
    "    all_interval_lengths = []\n",
    "    \n",
    "    # Initialize MAPIE with XGBoost regressor\n",
    "    regressor = xgb.XGBRegressor(**regression_params_short)\n",
    "    \n",
    "    # Fit regressor on non-zero training data\n",
    "    mask_nonzero = y_binary_train == 1\n",
    "    regressor.fit(X_train[mask_nonzero], y_cont_train[mask_nonzero])\n",
    "    \n",
    "    # Initialize MAPIE with the fitted regressor\n",
    "    mapie = MapieRegressor(\n",
    "        estimator=regressor,\n",
    "        method=\"plus\",\n",
    "        cv=\"prefit\",\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    for r in r_values:\n",
    "        # First calibration: threshold for zero/non-zero\n",
    "        alpha_r = np.quantile(probs_calib1, r)\n",
    "        \n",
    "        # Second calibration: accuracy of zero predictions\n",
    "        probs_calib2 = classifier.predict_proba(X_calib2)[:, 1]\n",
    "        pred_zeros = probs_calib2 <= alpha_r\n",
    "        beta_hat = np.mean(y_binary_calib2[pred_zeros] == 0)\n",
    "        \n",
    "        # Calculate final quantile and ensure it's in [0,1]\n",
    "        final_quantile = (alpha_tilda - beta_hat * r) / (1 - r)\n",
    "        final_quantile = np.clip(final_quantile, 0.01, 0.99)  # Clip to valid range\n",
    "        \n",
    "        # Third calibration: Use MAPIE for interval width\n",
    "        probs_calib3 = classifier.predict_proba(X_calib3)[:, 1]\n",
    "        nonzero_mask = probs_calib3 > alpha_r\n",
    "        if not any(nonzero_mask):\n",
    "            continue\n",
    "        \n",
    "        # Fit MAPIE on non-zero calibration data\n",
    "        X_calib3_nonzero = X_calib3[nonzero_mask]\n",
    "        y_calib3_nonzero = y_cont_calib3[nonzero_mask]\n",
    "        \n",
    "        try:\n",
    "            # Get MAPIE prediction intervals\n",
    "            mapie.fit(X_calib3_nonzero, y_calib3_nonzero)\n",
    "            _, y_pis = mapie.predict(X_calib3_nonzero, alpha=final_quantile)\n",
    "            \n",
    "            # Calculate interval width as mean of MAPIE intervals\n",
    "            interval_width = np.mean(y_pis[:, 1, 0] - y_pis[:, 0, 0])\n",
    "            all_interval_lengths.append((r, interval_width, alpha_r))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping r={r} due to error: {e}\")\n",
    "            continue\n",
    "    \n",
    "    if not all_interval_lengths:\n",
    "        raise ValueError(\"No valid intervals found. Try adjusting r_values or alpha_tilda.\")\n",
    "    \n",
    "    # Choose best r value\n",
    "    best_r, best_width, best_alpha_r = min(all_interval_lengths, key=lambda x: x[1])\n",
    "    \n",
    "    # Final predictions on test set\n",
    "    test_probs = classifier.predict_proba(X_test)[:, 1]\n",
    "    nonzero_mask_test = test_probs > best_alpha_r\n",
    "    \n",
    "    # Initialize bounds\n",
    "    lower_bound = np.zeros(len(X_test))\n",
    "    upper_bound = np.zeros(len(X_test))\n",
    "    \n",
    "    if any(nonzero_mask_test):\n",
    "        # Get MAPIE predictions for non-zero cases\n",
    "        _, y_pis_test = mapie.predict(X_test[nonzero_mask_test], alpha=0.9)  # Use fixed alpha for final prediction\n",
    "        lower_bound[nonzero_mask_test] = y_pis_test[:, 0, 0]\n",
    "        upper_bound[nonzero_mask_test] = y_pis_test[:, 1, 0]\n",
    "    \n",
    "    return lower_bound, upper_bound, y_cont_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPIE Coverage: 0.737\n",
      "MAPIE Average interval width: 0.001\n"
     ]
    }
   ],
   "source": [
    "lower_bound_mapie, upper_bound_mapie, y_test_mapie = conformal_prediction_zero_inflated_mapie(\n",
    "    X=X,\n",
    "    y_binary=y_pred_all,\n",
    "    y_continuous=normalized_pctg_change,\n",
    "    alpha_tilda=0.9\n",
    ")\n",
    "\n",
    "# Calculate coverage and interval width\n",
    "coverage_mapie = np.mean((y_test_mapie >= lower_bound_mapie) & (y_test_mapie <= upper_bound_mapie))\n",
    "interval_width_mapie = np.mean(upper_bound_mapie - lower_bound_mapie)\n",
    "print(f\"MAPIE Coverage: {coverage_mapie:.3f}\")\n",
    "print(f\"MAPIE Average interval width: {interval_width_mapie:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.offsetbox import AnnotationBbox, TextArea\n",
    "\n",
    "round_to = 2\n",
    "\n",
    "def sort_y_values(y_test, y_pred, y_pis):\n",
    "    \"\"\"\n",
    "    Sorting the dataset in order to make plots using the fill_between function.\n",
    "    \"\"\"\n",
    "    indices = np.argsort(y_test)\n",
    "    y_test_sorted = np.array(y_test)[indices]\n",
    "    y_pred_sorted = y_pred[indices]\n",
    "    y_lower_bound = y_pis[:, 0, 0][indices]\n",
    "    y_upper_bound = y_pis[:, 1, 0][indices]\n",
    "    return y_test_sorted, y_pred_sorted, y_lower_bound, y_upper_bound\n",
    "\n",
    "\n",
    "def plot_prediction_intervals(\n",
    "    title,\n",
    "    axs,\n",
    "    y_test_sorted,\n",
    "    y_pred_sorted,\n",
    "    lower_bound,\n",
    "    upper_bound,\n",
    "    coverage,\n",
    "    width,\n",
    "    num_plots_idx\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot of the prediction intervals for each different conformal\n",
    "    method.\n",
    "    \"\"\"\n",
    "    # axs.yaxis.set_major_formatter(FormatStrFormatter('%.0f' + \"k\"))\n",
    "    # axs.xaxis.set_major_formatter(FormatStrFormatter('%.0f' + \"k\"))\n",
    "\n",
    "    lower_bound_ = np.take(lower_bound, num_plots_idx)\n",
    "    y_pred_sorted_ = np.take(y_pred_sorted, num_plots_idx)\n",
    "    y_test_sorted_ = np.take(y_test_sorted, num_plots_idx)\n",
    "\n",
    "    error = y_pred_sorted_-lower_bound_\n",
    "\n",
    "    warning1 = y_test_sorted_ > y_pred_sorted_+error\n",
    "    warning2 = y_test_sorted_ < y_pred_sorted_-error\n",
    "    warnings = warning1 + warning2\n",
    "    axs.errorbar(\n",
    "        y_test_sorted_[~warnings],\n",
    "        y_pred_sorted_[~warnings],\n",
    "        yerr=np.abs(error[~warnings]),\n",
    "        capsize=5, marker=\"o\", elinewidth=2, linewidth=0,\n",
    "        label=\"Inside prediction interval\"\n",
    "        )\n",
    "    axs.errorbar(\n",
    "        y_test_sorted_[warnings],\n",
    "        y_pred_sorted_[warnings],\n",
    "        yerr=np.abs(error[warnings]),\n",
    "        capsize=5, marker=\"x\", elinewidth=2, linewidth=0, color=\"red\",\n",
    "        label=\"Outside prediction interval\"\n",
    "        )\n",
    "    axs.scatter(\n",
    "        y_test_sorted_[warnings],\n",
    "        y_test_sorted_[warnings],\n",
    "        marker=\"*\", color=\"green\",\n",
    "        label=\"True value\"\n",
    "    )\n",
    "    axs.set_xlabel(\"True Percent Change of LEDD\")\n",
    "    axs.set_ylabel(\"Prediction of Percent Change of LEDD\")\n",
    "    lims = [\n",
    "    np.min([axs.get_xlim(), axs.get_ylim()]),  # min of both axes\n",
    "    np.max([axs.get_xlim(), axs.get_ylim()]),  # max of both axes\n",
    "]\n",
    "    # lims = [-1,1]\n",
    "    ab = AnnotationBbox(\n",
    "    TextArea(\n",
    "        f\"Coverage: {np.round(coverage, round_to)}\\n\"\n",
    "        + f\"Interval width: {np.round(width, round_to)}\"\n",
    "    ),\n",
    "    xy=(lims[0] * 0.9, lims[1] * 0.9),  # Position annotation more relative to axis limits\n",
    "    xycoords='data',\n",
    "    boxcoords=\"offset points\",\n",
    "    frameon=True\n",
    "    )\n",
    "    axs.grid(True)\n",
    "    axs.set_xlim(lims)\n",
    "    axs.set_ylim(lims)\n",
    "    axs.plot(lims, lims, '--', alpha=0.75, color=\"black\", label=\"x=y\")\n",
    "    axs.add_artist(ab)\n",
    "    axs.set_title(title, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/mapie/utils.py:598: UserWarning: WARNING: The predictions are ill-sorted.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/mapie/utils.py:598: UserWarning: WARNING: The predictions are ill-sorted.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def conformal_prediction_zero_inflated_mapie(X, y_binary, y_continuous, alpha_tilda=0.9):\n",
    "    \"\"\"\n",
    "    Two-stage conformal prediction with multiple MAPIE strategies\n",
    "    \"\"\"\n",
    "    # Split data into five parts\n",
    "    (X_train, X_calib1, X_calib2, X_calib3, X_test,\n",
    "     y_binary_train, y_binary_calib1, y_binary_calib2, y_binary_calib3, y_binary_test,\n",
    "     y_cont_train, y_cont_calib1, y_cont_calib2, y_cont_calib3, y_cont_test) = split_into_five_equal_parts(\n",
    "        X, y_binary, y_continuous\n",
    "    )\n",
    "    \n",
    "    # Define MAPIE strategies like in your original code\n",
    "    STRATEGIES = {\n",
    "        \"naive\": {\"method\": \"naive\"},\n",
    "        \"cv_plus\": {\"method\": \"plus\", \"cv\": 10},\n",
    "        \"jackknife_plus_ab\": {\"method\": \"plus\", \"cv\": Subsample(n_resamplings=50)},\n",
    "    }\n",
    "    \n",
    "    # Initialize dictionaries to store results\n",
    "    y_pred, y_pis = {}, {}\n",
    "    y_test_sorted, y_pred_sorted, lower_bound, upper_bound = {}, {}, {}, {}\n",
    "    coverage, width = {}, {}\n",
    "    \n",
    "    # First stage: Classification model (common for all strategies)\n",
    "    classifier = xgb.XGBClassifier(**binary_params)\n",
    "    classifier.fit(X_train, y_binary_train)\n",
    "    \n",
    "    # Get probabilities for calibration set 1\n",
    "    probs_calib1 = classifier.predict_proba(X_calib1)[:, 1]\n",
    "    \n",
    "    # For each MAPIE strategy\n",
    "    for strategy, params in STRATEGIES.items():\n",
    "        # Initialize regressor\n",
    "        regressor = xgb.XGBRegressor(**regression_params_short)\n",
    "        \n",
    "        # Initialize MAPIE with current strategy\n",
    "        mapie = MapieRegressor(\n",
    "            estimator=regressor,\n",
    "            random_state=42,\n",
    "            **params\n",
    "        )\n",
    "        \n",
    "        # Try different r values\n",
    "        r_values = np.arange(0.1, 0.95, 0.05)\n",
    "        all_interval_lengths = []\n",
    "        \n",
    "        for r in r_values:\n",
    "            # First calibration: threshold for zero/non-zero\n",
    "            alpha_r = np.quantile(probs_calib1, r)\n",
    "            \n",
    "            # Second calibration: accuracy of zero predictions\n",
    "            probs_calib2 = classifier.predict_proba(X_calib2)[:, 1]\n",
    "            pred_zeros = probs_calib2 <= alpha_r\n",
    "            beta_hat = np.mean(y_binary_calib2[pred_zeros] == 0)\n",
    "            \n",
    "            # Calculate final quantile and ensure it's in [0,1]\n",
    "            final_quantile = (alpha_tilda - beta_hat * r) / (1 - r)\n",
    "            final_quantile = np.clip(final_quantile, 0.01, 0.99)\n",
    "            \n",
    "            # Third calibration: Use MAPIE for interval width\n",
    "            probs_calib3 = classifier.predict_proba(X_calib3)[:, 1]\n",
    "            nonzero_mask = probs_calib3 > alpha_r\n",
    "            \n",
    "            if not any(nonzero_mask):\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Fit MAPIE on non-zero data\n",
    "                X_calib3_nonzero = X_calib3[nonzero_mask]\n",
    "                y_calib3_nonzero = y_cont_calib3[nonzero_mask]\n",
    "                \n",
    "                mapie.fit(X_calib3_nonzero, y_calib3_nonzero)\n",
    "                _, y_pis_calib = mapie.predict(X_calib3_nonzero, alpha=final_quantile)\n",
    "                \n",
    "                # Calculate interval width\n",
    "                interval_width = np.mean(y_pis_calib[:, 1, 0] - y_pis_calib[:, 0, 0])\n",
    "                all_interval_lengths.append((r, interval_width, alpha_r, mapie))\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Skipping {strategy} with r={r} due to error: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not all_interval_lengths:\n",
    "            print(f\"No valid intervals found for strategy {strategy}\")\n",
    "            continue\n",
    "        \n",
    "        # Choose best r value and corresponding MAPIE model\n",
    "        best_r, best_width, best_alpha_r, best_mapie = min(all_interval_lengths, key=lambda x: x[1])\n",
    "        \n",
    "        # Final predictions on test set\n",
    "        test_probs = classifier.predict_proba(X_test)[:, 1]\n",
    "        nonzero_mask_test = test_probs > best_alpha_r\n",
    "        \n",
    "        # Initialize bounds\n",
    "        lower_bound[strategy] = np.zeros(len(X_test))\n",
    "        upper_bound[strategy] = np.zeros(len(X_test))\n",
    "        \n",
    "        if any(nonzero_mask_test):\n",
    "            _, y_pis_test = best_mapie.predict(X_test[nonzero_mask_test], alpha=alpha_tilda)\n",
    "            lower_bound[strategy][nonzero_mask_test] = y_pis_test[:, 0, 0]\n",
    "            upper_bound[strategy][nonzero_mask_test] = y_pis_test[:, 1, 0]\n",
    "        \n",
    "        # Calculate coverage and width using MAPIE metrics\n",
    "        coverage[strategy] = regression_coverage_score(\n",
    "            y_cont_test,\n",
    "            lower_bound[strategy],\n",
    "            upper_bound[strategy]\n",
    "        )\n",
    "        width[strategy] = regression_mean_width_score(\n",
    "            lower_bound[strategy],\n",
    "            upper_bound[strategy]\n",
    "        )\n",
    "    \n",
    "    return lower_bound, upper_bound, y_cont_test, coverage, width\n",
    "\n",
    "# Run the analysis\n",
    "results = conformal_prediction_zero_inflated_mapie(\n",
    "    X=X,\n",
    "    y_binary=y_pred_all,\n",
    "    y_continuous=normalized_pctg_change,\n",
    "    alpha_tilda=0.9\n",
    ")\n",
    "\n",
    "lower_bound, upper_bound, y_test, coverage, width = results\n",
    "\n",
    "# Print results for each strategy\n",
    "for strategy in coverage.keys():\n",
    "    print(f\"\\nStrategy: {strategy}\")\n",
    "    print(f\"Coverage: {coverage[strategy]:.3f}\")\n",
    "    print(f\"Average interval width: {width[strategy]:.3f}\")\n",
    "\n",
    "# Visualization similar to your original code\n",
    "rng = np.random.default_rng(21)\n",
    "perc_obs_plot = 0.90\n",
    "num_plots = rng.choice(len(y_test), int(perc_obs_plot*len(y_test)), replace=False)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "coords = axs.ravel()\n",
    "\n",
    "for strategy, coord in zip(lower_bound.keys(), coords):\n",
    "    plot_prediction_intervals(\n",
    "        strategy, coord, y_test, (lower_bound[strategy] + upper_bound[strategy])/2,\n",
    "        lower_bound[strategy], upper_bound[strategy],\n",
    "        coverage[strategy], width[strategy],\n",
    "        num_plots\n",
    "    )\n",
    "    coord.grid(True, linestyle='--', alpha=0.4, linewidth=0.8, color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
